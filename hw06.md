# 计算机组成/概念解释（Wikipedia）

### 1、According to the von Neumann architecture,List basic parts of a computer.

- Input device
- Central Processing Unit : Control unit ; Arithmetic/logic unit
- Memory unit
- Auxiliary storage device
- Output device

### 2、A computer has 64 MB (megabytes) of memory. How many bits are needed to address any single byte in memory?

26

### 3、List basic parts of a CPU, include cache or not?

Not

### 4、What mean secondary storage. List some on your PC.

Secondary storage is a non-volatile memory (does not lose stored data when the device is powered down) that is not directly accessible by the CPU, because it is not accessed via the input/output channels (it is an external device).

128G SSD 固态硬盘

2T 机械硬盘


### 5、使用维基百科，解释以下概念：CPU、RAM、ROM、Bus (computing)、Parallel Computing

**CPU** : A central processing unit (CPU) is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control and input/output (I/O) operations specified by the instructions. The computer industry has used the term "central processing unit" at least since the early 1960s. Traditionally, the term "CPU" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.

The form, design, and implementation of CPUs have changed over the course of their history, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations and a control unit that orchestrates the fetching (from memory) and execution of instructions by directing the coordinated operations of the ALU, registers and other components.

Most modern CPUs are microprocessors, meaning they are contained on a single integrated circuit (IC) chip. An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). Some computers employ a multi-core processor, which is a single chip containing two or more CPUs called "cores"; in that context, one can speak of such single chips as "sockets".

**RAM** : Random-access memory (RAM) is a form of computer data storage that stores data and machine code currently being used. A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory. In contrast, with other direct-access data storage media such as hard disks, CD-RWs, DVD-RWs and the older magnetic tapes and drum memory, the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.

RAM contains multiplexing and demultiplexing circuitry, to connect the data lines to the addressed storage for reading or writing the entry. Usually more than one bit of storage is accessed by the same address, and RAM devices often have multiple data lines and are said to be "8-bit" or "16-bit", etc. devices.

In today's technology, random-access memory takes the form of integrated circuits. RAM is normally associated with volatile types of memory (such as DRAM modules), where stored information is lost if power is removed, although non-volatile RAM has also been developed. Other types of non-volatile memories exist that allow random access for read operations, but either do not allow write operations or have other kinds of limitations on them. These include most types of ROM and a type of flash memory called NOR-Flash.

**ROM** : Read-only memory (ROM) is a type of non-volatile memory used in computers and other electronic devices. Data stored in ROM can only be modified slowly, with difficulty, or not at all, so it is mainly used to store firmware (software that is closely tied to specific hardware, and unlikely to need frequent updates) or application software in plug-in cartridges.

Strictly, read-only memory refers to memory that is hard-wired, such as diode matrix and the later mask ROM (MROM), which cannot be changed after manufacture. Although discrete circuits can be altered in principle, integrated circuits (ICs) cannot, and are useless if the data is bad or requires an update. That such memory can never be changed is a disadvantage in many applications, as bugs and security issues cannot be fixed, and new features cannot be added.

More recently, ROM has come to include memory that is read-only in normal operation, but can still be reprogrammed in some way. Erasable programmable read-only memory (EPROM) and electrically erasable programmable read-only memory (EEPROM) can be erased and re-programmed, but usually this can only be done at relatively slow speeds, may require special equipment to achieve, and is typically only possible a certain number of times.

**Bus (computing)** : a bus (a contraction of the Latin omnibus) is a communication system that transfers data between components inside a computer, or between computers. This expression covers all related hardware components (wire, optical fiber, etc.) and software, including communication protocols.

Early computer buses were parallel electrical wires with multiple hardware connections, but the term is now used for any physical arrangement that provides the same logical function as a parallel electrical bus. Modern computer buses can use both parallel and bit serial connections, and can be wired in either a multidrop (electrical parallel) or daisy chain topology, or connected by switched hubs, as in the case of USB.

**Parallel Computing** : Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but it's gaining broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.

Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.

Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.

In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.

A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.

### 6、写一段文字，简单解释“云计算（cloud computing）”

云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络访问， 进入可配置的计算资源共享池（资源包括网络，服务器，存储，应用软件，服务），这些资源能够被快速提供，只需投入很少的管理工作，或与服务供应商进行很少的交互。

云计算是通过使计算分布在大量的分布式计算机上，而非本地计算机或远程服务器中，企业数据中心的运行将与互联网更相似。这使得企业能够将资源切换到需要的应用上，根据需求访问计算机和存储系统。

被普遍接受的云计算特点有：超大规模、虚拟化、高可靠性、通用性、高可扩展性、按需服务、极其廉价、潜在的危险性

### 7、小孙买了计算机主板，说明书表明“支持双通道DDR3-1333内存，最大支持16G” 

#### 1）DDR3内存，“3”和“1333”的含义是什么？

3表示是第三代双倍数据率同步动态随机存取存储器。

1333指内存主频是1333MHz。

内存主频和CPU主频一样，习惯上被用来表示内存的速度，它代表着该内存所能达到的最高工作的频率。内存主频是以MHz（兆赫）为单位来计量的。内存主频越高在一定程度上代表着内存所能达到的速度越快。内存主频决定着该内存最高能在什么样的频率正常工作。

#### 2）小孙买8G DDR3-1600的内存能提高性能吗？

不能

#### 3）小孙买4G*2 DDR3-1333的内存能提高性能吗？

能

#### 4）16G需要多少位地址？

34

#### 备注，维基百科（中文）“DDR3 SDRAM”

第三代双倍数据率同步动态随机存取存储器（Double-Data-Rate Three Synchronous Dynamic Random Access Memory，一般称为DDR3 SDRAM），是一种计算机存储器规格。它属于SDRAM家族的存储器产品，提供相较于DDR2 SDRAM更高的运行性能与更低的电压，是DDR2 SDRAM（四倍数据率同步动态随机存取存储器）的后继者（增加至八倍）。

DDR3 SDRAM为了更省电、传输效率更快，使用SSTL 15的I/O接口，运作I/O电压是1.5V，采用CSP、FBGA封装方式包装，除了延续DDR2 SDRAM的ODT、OCD、Posted CAS、AL控制方式外，另外新增更为精进的CWD、Reset、ZQ、SRT、PASR功能。

CWD是作为写入延迟之用，Reset提供超省电功能的命令，可以让DDR3 SDRAM存储器颗粒电路停止运作、进入超省电待命模式，ZQ则是一个新增的终端电阻校准功能，新增这个线路脚位提供了ODCE（On Die Calibration Engine）用来校准ODT（On Die Termination）内部终端电阻，新增SRT（Self-Reflash Temperature）可编程温度控制存储器时钟频率功能，SRT的加入让存储器颗粒在温度、时钟频率和电源管理上进行优化，可以说在存储器内，就做了电源管理的功能，同时让存储器颗粒的稳定度也大为提升，确保存储器颗粒不致于工作时钟频率过高导致烧毁的状况，同时DDR3 SDRAM还加入PASR（Partial Array Self-Refresh）局部Bank刷新的功能，可以说针对整个存储器Bank做更有效的数据读写以达到省电功效。